import os
import re
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
from diffusers import UNet2DModel, DDPMScheduler
from tqdm import tqdm
from torchvision.utils import save_image
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingLR
import gc
import random

def get_version(filename: str) -> str:
    match = re.search(r'V[3-7]', filename)
    return match.group(0) if match else "Unknown"

# ì™„ì „í•œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„± (í´ë¦¬í•‘ ì—†ìŒ)
def generate_gaussian_noise_pair(img_path, image_size=512, noise_std=0.1):
    img = Image.open(img_path).convert('L').resize((image_size, image_size))
    img_array = np.array(img) / 255.0

    # í‘œì¤€ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ (í´ë¦¬í•‘ ì—†ìŒ)
    noise = np.random.normal(0, noise_std, size=img_array.shape)
    noisy_img = img_array + noise

    return noisy_img.astype(np.float32), img_array.astype(np.float32)

class GaussianNoiseDataset(Dataset):
    def __init__(self, image_dir, image_size=512, noise_std=0.1):
        self.image_dir = image_dir
        self.image_files = sorted(os.listdir(image_dir))
        self.image_size = image_size
        self.noise_std = noise_std

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        path = os.path.join(self.image_dir, self.image_files[idx])

        # ì™„ì „í•œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±
        noisy_img, clean_img = generate_gaussian_noise_pair(
            path, self.image_size, noise_std=self.noise_std
        )

        # ì •ê·œí™”: [0,1] -> [-1,1] (í´ë¦¬í•‘ ì—†ì´)
        noisy_tensor = torch.tensor(noisy_img * 2.0 - 1.0, dtype=torch.float32).unsqueeze(0).repeat(3, 1, 1)
        clean_tensor = torch.tensor(clean_img * 2.0 - 1.0, dtype=torch.float32).unsqueeze(0).repeat(3, 1, 1)

        version = get_version(self.image_files[idx])
        return noisy_tensor, clean_tensor, version

# ì„¤ì •
image_dir = "data/train_gt"
output_dir = "ddpm_checkpoints_gaussian_pure"
os.makedirs(output_dir, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
epochs = 100
batch_size = 1
lr = 1e-4
image_size = 512
noise_std = 0.1  # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨

print(f"Using device: {device}")
print(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB" if torch.cuda.is_available() else "CPU mode")

# ëª¨ë¸ ì •ì˜ - ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì œê±°ìš© UNet
model = UNet2DModel(
    sample_size=image_size,
    in_channels=3,
    out_channels=3,
    layers_per_block=2,
    block_out_channels=(128, 256, 512, 512, 1024),
    down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "DownBlock2D"),
    up_block_types=("UpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D"),
    attention_head_dim=8,
).to(device)

# í‘œì¤€ DDPM ìŠ¤ì¼€ì¤„ëŸ¬
noise_scheduler = DDPMScheduler(
    num_train_timesteps=1000,
    beta_schedule="scaled_linear",
    prediction_type="epsilon"
)

optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr/10)

# Mixed precisionì„ ìœ„í•œ scaler
scaler = GradScaler()

# ìˆœìˆ˜ ê°€ìš°ì‹œì•ˆ ë°ì´í„° ë¡œë”© (ì¦ê°• ì—†ìŒ)
dataset = GaussianNoiseDataset(image_dir, image_size, noise_std=noise_std)
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
    pin_memory=True,
    persistent_workers=True,
    drop_last=True
)

print(f"Dataset size: {len(dataset)}")
print(f"Batches per epoch: {len(dataloader)}")
print(f"Gaussian noise std: {noise_std}")

# ë²„ì „ë³„ ë¶„í¬ í™•ì¸
version_counts = {}
for file in dataset.image_files:
    version = get_version(file)
    version_counts[version] = version_counts.get(version, 0) + 1
print(f"Version distribution: {version_counts}")

# í•™ìŠµ ë£¨í”„
model.train()
best_loss = float('inf')

for epoch in range(epochs):
    print(f"ğŸŒŸ Epoch {epoch+1}/{epochs} (LR: {scheduler.get_last_lr()[0]:.2e})")

    epoch_loss = 0.0
    version_losses = {}
    progress_bar = tqdm(dataloader, desc=f"Training")

    for batch_idx, (noisy, clean, versions) in enumerate(progress_bar):
        noisy, clean = noisy.to(device, non_blocking=True), clean.to(device, non_blocking=True)

        # DDPM ë…¸ì´ì¦ˆ ì¶”ê°€
        diffusion_noise = torch.randn_like(clean)
        timesteps = torch.randint(
            0, noise_scheduler.config.num_train_timesteps,
            (clean.shape[0],), device=device
        ).long()

        # Mixed precisionìœ¼ë¡œ forward pass
        with autocast():
            noisy_diffusion = noise_scheduler.add_noise(clean, diffusion_noise, timesteps)
            pred_noise = model(noisy_diffusion, timesteps).sample
            loss = torch.nn.functional.mse_loss(pred_noise, diffusion_noise)

        # Backward pass with gradient scaling
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()

        epoch_loss += loss.item()

        # ë²„ì „ë³„ ì†ì‹¤ ì¶”ì 
        for version in versions:
            if version not in version_losses:
                version_losses[version] = []
            version_losses[version].append(loss.item())

        progress_bar.set_postfix({'loss': f'{loss.item():.6f}'})

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if batch_idx % 50 == 0:
            torch.cuda.empty_cache()

    avg_loss = epoch_loss / len(dataloader)
    print(f"Average loss: {avg_loss:.6f}")

    # ë²„ì „ë³„ í‰ê·  ì†ì‹¤ ì¶œë ¥
    for version, losses in version_losses.items():
        avg_version_loss = sum(losses) / len(losses)
        print(f"  {version} avg loss: {avg_version_loss:.6f} ({len(losses)} samples)")

    scheduler.step()

    # í…ŒìŠ¤íŠ¸ (ë§¤ 5 ì—í¬í¬ë§ˆë‹¤)
    if (epoch + 1) % 5 == 0:
        model.eval()
        with torch.no_grad():
            print("Testing denoising...")

            # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°
            test_noisy, test_clean, test_version = dataset[0]
            test_noisy = test_noisy.unsqueeze(0).to(device)

            # DDPM ë””ë…¸ì´ì§•
            denoised = test_noisy.clone()
            for t in tqdm(reversed(range(noise_scheduler.config.num_train_timesteps)),
                         desc="Denoising", leave=False):
                with autocast():
                    noise_pred = model(denoised, torch.tensor([t], device=device)).sample
                denoised = noise_scheduler.step(noise_pred, t, denoised).prev_sample

            # ê²°ê³¼ ì €ì¥
            comparison = torch.cat([test_noisy, denoised, test_clean.unsqueeze(0).to(device)], dim=0)
            save_image(comparison,
                      os.path.join(output_dir, f"comparison_epoch{epoch+1}_{test_version}.png"),
                      normalize=True, value_range=(-1, 1), nrow=3)

        model.train()

    # ëª¨ë¸ ì €ì¥
    if (epoch + 1) % 10 == 0 or avg_loss < best_loss:
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_loss,
                'version_losses': version_losses,
                'noise_std': noise_std,
            }, os.path.join(output_dir, f"best_model_gaussian.pt"))
            print(f"ğŸ’¾ Best model saved! Loss: {avg_loss:.6f}")

        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'loss': avg_loss,
            'version_losses': version_losses,
            'noise_std': noise_std,
        }, os.path.join(output_dir, f"ddpm_epoch{epoch+1}_gaussian.pt"))

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    torch.cuda.empty_cache()

print("ğŸ‰ Training completed!")
print(f"Best loss achieved: {best_loss:.6f}")
print(f"Models saved in: {output_dir}")

# ìµœì¢… í†µê³„
print("\nğŸ“Š Final statistics:")
print(f"Total training samples: {len(dataset)}")
print(f"Gaussian noise std: {noise_std}")
print(f"Original images by version: {version_counts}")